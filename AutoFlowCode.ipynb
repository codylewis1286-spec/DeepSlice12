{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 1\n",
    "#Imports\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sns.set(style=\"whitegrid\")  # GraphPad-ish background\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c9cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "# --- Excel File Finder ---\n",
    "DATA_DIR = Path(r\"C:\\Users\\M298134\\Desktop\\AutoFlow\\Tyler's Work\")  # <-- change this if needed\n",
    "\n",
    "def get_latest_excel_file(data_dir: Path) -> Path | None:\n",
    "    excel_files = list(data_dir.glob(\"*.xlsx\")) + list(data_dir.glob(\"*.xls\"))\n",
    "    if not excel_files:\n",
    "        return None\n",
    "    # sort by modification time (newest last)\n",
    "    excel_files.sort(key=lambda f: f.stat().st_mtime)\n",
    "    return excel_files[-1]\n",
    "\n",
    "latest_file = get_latest_excel_file(DATA_DIR)\n",
    "\n",
    "if latest_file is None:\n",
    "    raise FileNotFoundError(f\"No Excel files found in {DATA_DIR}\")\n",
    "else:\n",
    "    print(f\"Using file: {latest_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b89ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 \n",
    "# Read Table\n",
    "df_raw = pd.read_excel(latest_file)\n",
    "display(df_raw.head())\n",
    "\n",
    "print(\"Columns:\", df_raw.columns.tolist())  # handy to see once\n",
    "\n",
    "def extract_group_from_sample(sample_name: str) -> str | None:\n",
    "    if not isinstance(sample_name, str):\n",
    "        return None\n",
    "    m = re.match(r\"([A-Za-z]+)\\d+\", sample_name.strip())\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "SAMPLE_COL = \"Sample:\"  # <-- the actual column name from your file\n",
    "\n",
    "if SAMPLE_COL not in df_raw.columns:\n",
    "    raise KeyError(\n",
    "        f\"Expected a sample ID column named '{SAMPLE_COL}' in the Excel file. \"\n",
    "        f\"Found: {df_raw.columns.tolist()}\"\n",
    "    )\n",
    "\n",
    "df = df_raw.copy()\n",
    "df[\"Group\"] = df[SAMPLE_COL].apply(extract_group_from_sample)\n",
    "\n",
    "df[[SAMPLE_COL, \"Group\"]].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 – Smart FlowJo column renaming\n",
    "\n",
    "def clean_marker_name(m: str) -> str:\n",
    "    \"\"\"\n",
    "    Light cleanup of marker names.\n",
    "    You can expand this as you see patterns (e.g. CD8a+ -> CD8+).\n",
    "    \"\"\"\n",
    "    # Simple tweaks that are common:\n",
    "    m = m.replace(\"CD8a\", \"CD8\")\n",
    "    m = m.replace(\"CD4 \", \"CD4\")  # remove stray spaces\n",
    "    return m\n",
    "\n",
    "def make_nice_flowjo_name(col: str) -> str:\n",
    "    \"\"\"\n",
    "    Turn a long FlowJo column name into something like:\n",
    "    'VP2+ CD8+ Freq. of Parent'\n",
    "    \"\"\"\n",
    "    # Keep some special columns as-is\n",
    "    if col in [\"Sample:\", \"Group\"]:\n",
    "        return col.replace(\":\", \"\")  # 'Sample:' -> 'Sample'\n",
    "    \n",
    "    # If there's no '|' separator, just return the column\n",
    "    if \"|\" not in col:\n",
    "        return col\n",
    "\n",
    "    gate_part, metric_part = col.split(\"|\", 1)\n",
    "    gate_part = gate_part.strip()\n",
    "    metric_part = metric_part.strip()  # e.g. 'Freq. of Parent'\n",
    "\n",
    "    # Find all tokens that look like markers ending in + or -\n",
    "    # e.g. CD8a+, VP2+, CD4-, PD-1+\n",
    "    markers = re.findall(r\"([A-Za-z0-9\\-]+[+\\-])\", gate_part)\n",
    "\n",
    "    # Clean up marker names a bit\n",
    "    markers = [clean_marker_name(m) for m in markers]\n",
    "\n",
    "    # If we got at least two markers, use the last two\n",
    "    if len(markers) >= 2:\n",
    "        m1 = markers[-1]      # last (e.g. VP2+)\n",
    "        m2 = markers[-2]      # second to last (e.g. CD8+)\n",
    "        nice = f\"{m1} {m2} {metric_part}\"\n",
    "    elif len(markers) == 1:\n",
    "        m1 = markers[0]\n",
    "        nice = f\"{m1} {metric_part}\"\n",
    "    else:\n",
    "        # Fallback: just use the metric if no markers were detected\n",
    "        nice = metric_part\n",
    "\n",
    "    return nice\n",
    "\n",
    "def shorten_column_names(columns):\n",
    "    return {c: make_nice_flowjo_name(c) for c in columns}\n",
    "\n",
    "# Apply to your df (which already has Group)\n",
    "df_renamed = df.rename(columns=shorten_column_names(df.columns))\n",
    "\n",
    "print(\"Old -> New column names (first 10):\")\n",
    "for old, new in list(shorten_column_names(df.columns).items())[:10]:\n",
    "    print(f\"{old!r} -> {new!r}\")\n",
    "\n",
    "df_renamed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ede18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# Summarize by group\n",
    "\n",
    "# Only keep numeric columns for stats\n",
    "numeric_cols = df_renamed.select_dtypes(include=\"number\").columns\n",
    "\n",
    "group_summary = (\n",
    "    df_renamed\n",
    "    .groupby(\"Group\")[numeric_cols]\n",
    "    .agg([\"mean\", \"std\", \"median\", \"count\"])\n",
    ")\n",
    "\n",
    "group_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d524756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 – Robust Stats Analysis (Outliers, Normality, Variance Tests, Correct Stats Choice)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import shapiro, levene, bartlett, f_oneway, kruskal\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def detect_outliers_grubbs(values, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Apply Grubbs' test for one outlier at a time.\n",
    "    Returns indices of outliers.\n",
    "    Requires n >= 3.\n",
    "    \"\"\"\n",
    "    vals = values.copy().astype(float)\n",
    "    outliers = []\n",
    "    while len(vals) >= 3:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals, ddof=1)\n",
    "        if std == 0:\n",
    "            break\n",
    "        G = np.max(np.abs(vals - mean)) / std\n",
    "\n",
    "        # Critical value for Grubbs (two-sided)\n",
    "        n = len(vals)\n",
    "        t_crit = stats.t.ppf(1 - alpha/(2*n), n - 2)\n",
    "        G_crit = ((n - 1) / np.sqrt(n)) * np.sqrt(t_crit**2 / (n - 2 + t_crit**2))\n",
    "\n",
    "        if G > G_crit:\n",
    "            outlier_value = vals[np.argmax(np.abs(vals - mean))]\n",
    "            outliers.append(outlier_value)\n",
    "            vals = vals[vals != outlier_value]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return outliers\n",
    "\n",
    "\n",
    "def variable_stats(df, variable, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform:\n",
    "      - Outlier detection\n",
    "      - Normality tests\n",
    "      - Variance equality tests\n",
    "      - Appropriate group comparison (ANOVA, Welch, or Kruskal)\n",
    "    \"\"\"\n",
    "    result = {\"variable\": variable}\n",
    "\n",
    "    # ----------- Gather values per group -----------\n",
    "    groups = sorted(df[\"Group\"].dropna().unique())\n",
    "    group_vals = {g: df.loc[df[\"Group\"] == g, variable].dropna().values for g in groups}\n",
    "\n",
    "    # Must have ≥2 groups with data\n",
    "    usable = [g for g in groups if len(group_vals[g]) >= 2]\n",
    "    if len(usable) < 2:\n",
    "        result.update({\"test_used\": \"Insufficient data\", \"p\": np.nan})\n",
    "        return result\n",
    "\n",
    "    # ----------- Outlier detection -----------\n",
    "    outliers_removed = {}\n",
    "    cleaned_vals = {}\n",
    "\n",
    "    for g in usable:\n",
    "        vals = group_vals[g]\n",
    "        outs = detect_outliers_grubbs(vals, alpha=0.05)\n",
    "        outliers_removed[g] = outs\n",
    "        cleaned_vals[g] = np.array([v for v in vals if v not in outs])\n",
    "\n",
    "    result[\"outliers_removed\"] = outliers_removed\n",
    "\n",
    "    # ----------- Normality tests (Shapiro-Wilk) -----------\n",
    "    normal = []\n",
    "    for g in usable:\n",
    "        if len(cleaned_vals[g]) >= 3:\n",
    "            p_norm = shapiro(cleaned_vals[g])[1]\n",
    "            normal.append(p_norm > alpha)\n",
    "        else:\n",
    "            normal.append(False)\n",
    "\n",
    "    result[\"normality_pass\"] = all(normal)\n",
    "\n",
    "    # ----------- Variance equality tests -----------\n",
    "    val_list = [cleaned_vals[g] for g in usable]\n",
    "    p_levene = levene(*val_list)[1]\n",
    "    p_bartlett = bartlett(*val_list)[1]\n",
    "\n",
    "    result[\"levene_equal_var\"] = p_levene > alpha\n",
    "    result[\"bartlett_equal_var\"] = p_bartlett > alpha\n",
    "\n",
    "    # ----------- Choose test -----------\n",
    "    if result[\"normality_pass\"]:\n",
    "        if p_levene > alpha:\n",
    "            test_name = \"ANOVA\"\n",
    "            stat, p_value = f_oneway(*val_list)\n",
    "        else:\n",
    "            test_name = \"Welch ANOVA\"\n",
    "            # SciPy does not have Welch ANOVA natively → do Kruskal fallback OR ping me for implementation\n",
    "            stat, p_value = kruskal(*val_list)\n",
    "    else:\n",
    "        test_name = \"Kruskal–Wallis\"\n",
    "        stat, p_value = kruskal(*val_list)\n",
    "\n",
    "    result[\"test_used\"] = test_name\n",
    "    result[\"p\"] = p_value\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ----------- Run stats for all variables -----------\n",
    "\n",
    "stats_results = []\n",
    "for var in numeric_cols:\n",
    "    stats_results.append(variable_stats(df_renamed, var))\n",
    "\n",
    "stats_df = pd.DataFrame(stats_results)\n",
    "\n",
    "# Multiple-test correction (Benjamini–Hochberg FDR)\n",
    "mask = stats_df[\"p\"].notna()\n",
    "reject, p_adj, *_ = multipletests(stats_df.loc[mask, \"p\"], alpha=0.05, method=\"fdr_bh\")\n",
    "stats_df.loc[mask, \"p_adj\"] = p_adj\n",
    "stats_df.loc[mask, \"significant_fdr<0.05\"] = reject\n",
    "\n",
    "# Sort by adjusted p-value\n",
    "stats_df.sort_values(\"p_adj\", inplace=True)\n",
    "\n",
    "display(stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 – GraphPad-style plots ONLY for significant variables\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "def get_sig_star(p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'ns'\n",
    "\n",
    "sig_vars = anova_df.loc[anova_df[\"p\"] < alpha, \"variable\"].tolist()\n",
    "\n",
    "print(f\"Significant variables (p < {alpha}):\")\n",
    "for v in sig_vars:\n",
    "    print(\"  -\", v)\n",
    "\n",
    "if not sig_vars:\n",
    "    print(\"No significant variables found.\")\n",
    "else:\n",
    "    # choose a reference group for pairwise comparisons (like GraphPad)\n",
    "    all_groups = sorted(df_renamed[\"Group\"].dropna().unique())\n",
    "    ref_group = all_groups[0]   # e.g. use 'A' as control; change if you want\n",
    "\n",
    "    print(f\"\\nUsing '{ref_group}' as reference for pairwise tests.\")\n",
    "\n",
    "    for TARGET_VAR in sig_vars:\n",
    "        print(f\"\\nPlotting significant variable: {TARGET_VAR}\")\n",
    "        \n",
    "        # long-format data for seaborn\n",
    "        plot_df = df_renamed[[\"Group\", TARGET_VAR]].dropna().copy()\n",
    "        plot_df = plot_df[plot_df[\"Group\"].notna()]\n",
    "\n",
    "        plt.figure(figsize=(6, 5))\n",
    "\n",
    "        # Boxplot + swarm of individual points\n",
    "        ax = sns.boxplot(\n",
    "            data=plot_df,\n",
    "            x=\"Group\",\n",
    "            y=TARGET_VAR,\n",
    "            palette=\"Set2\",\n",
    "            showcaps=True,\n",
    "            boxprops={\"linewidth\": 1.5},\n",
    "            whiskerprops={\"linewidth\": 1.5},\n",
    "            medianprops={\"linewidth\": 1.5},\n",
    "        )\n",
    "        sns.swarmplot(\n",
    "            data=plot_df,\n",
    "            x=\"Group\",\n",
    "            y=TARGET_VAR,\n",
    "            color=\"0.25\",\n",
    "            size=5\n",
    "        )\n",
    "\n",
    "        # Basic labels\n",
    "        plt.xlabel(\"Group\")\n",
    "        plt.ylabel(TARGET_VAR)\n",
    "        overall_p = anova_df.loc[anova_df[\"variable\"] == TARGET_VAR, \"p\"].values[0]\n",
    "        plt.title(f\"{TARGET_VAR}\\nANOVA p = {overall_p:.3e}\")\n",
    "\n",
    "        # ---- Add pairwise significance vs reference group ----\n",
    "        # we'll compare ref_group vs every other group\n",
    "        x_positions = {g: i for i, g in enumerate(all_groups)}\n",
    "        y_max = plot_df[TARGET_VAR].max()\n",
    "        y_min = plot_df[TARGET_VAR].min()\n",
    "        y_range = y_max - y_min if y_max > y_min else 1.0\n",
    "\n",
    "        # start a bit above the max data\n",
    "        current_height = y_max + 0.1 * y_range\n",
    "        step = 0.08 * y_range  # vertical spacing between brackets\n",
    "\n",
    "        for g in all_groups:\n",
    "            if g == ref_group:\n",
    "                continue\n",
    "\n",
    "            vals_ref = plot_df.loc[plot_df[\"Group\"] == ref_group, TARGET_VAR]\n",
    "            vals_g   = plot_df.loc[plot_df[\"Group\"] == g, TARGET_VAR]\n",
    "\n",
    "            if len(vals_ref) < 2 or len(vals_g) < 2:\n",
    "                continue  # skip tiny groups\n",
    "\n",
    "            t, p_pair = stats.ttest_ind(vals_ref, vals_g, nan_policy=\"omit\")\n",
    "            star = get_sig_star(p_pair)\n",
    "\n",
    "            if star == 'ns':\n",
    "                continue  # don't draw NS comparisons\n",
    "\n",
    "            x1 = x_positions[ref_group]\n",
    "            x2 = x_positions[g]\n",
    "            x_center = (x1 + x2) / 2\n",
    "\n",
    "            # draw a bracket\n",
    "            ax.plot(\n",
    "                [x1, x1, x2, x2],\n",
    "                [current_height, current_height + step/2, current_height + step/2, current_height],\n",
    "                color=\"black\",\n",
    "                linewidth=1.2,\n",
    "            )\n",
    "            ax.text(\n",
    "                x_center,\n",
    "                current_height + step/2 + 0.02 * y_range,\n",
    "                star,\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=12,\n",
    "            )\n",
    "\n",
    "            current_height += step  # move up for next comparison\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb07d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 – Build GraphPad-style wide table (columns = groups, rows = replicates)\n",
    "\n",
    "def make_graphpad_block(var: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a column name `var` from df_renamed, returns a wide DataFrame:\n",
    "      - columns = groups (A, B, C, ...)\n",
    "      - rows = replicates (1, 2, 3, ...)\n",
    "    \"\"\"\n",
    "    if var not in df_renamed.columns:\n",
    "        raise KeyError(f\"{var!r} is not a column in df_renamed. \"\n",
    "                       f\"Available: {df_renamed.columns.tolist()}\")\n",
    "\n",
    "    sub = df_renamed[[\"Group\", var]].dropna().copy()\n",
    "    sub = sub[sub[\"Group\"].notna()]\n",
    "\n",
    "    # Add replicate index within each group\n",
    "    sub[\"rep\"] = sub.groupby(\"Group\").cumcount()\n",
    "\n",
    "    # Pivot: each group becomes its own column\n",
    "    wide = sub.pivot(index=\"rep\", columns=\"Group\", values=var)\n",
    "\n",
    "    # Order groups (A, B, C, …)\n",
    "    wide = wide.reindex(sorted(wide.columns), axis=1)\n",
    "\n",
    "    # Replicates start at 1 (nicer to look at)\n",
    "    wide.index = wide.index + 1\n",
    "    wide.index.name = None\n",
    "\n",
    "    return wide\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e5a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 – Single-sheet Excel with labeled blocks for all significant variables\n",
    "\n",
    "alpha = 0.05\n",
    "sig_vars = anova_df.loc[anova_df[\"p\"] < alpha, \"variable\"].tolist()\n",
    "\n",
    "print(f\"Found {len(sig_vars)} significant variables (p < {alpha}).\")\n",
    "\n",
    "if not sig_vars:\n",
    "    print(\"No significant variables to export.\")\n",
    "else:\n",
    "    blocks_for_concat = []\n",
    "\n",
    "    for var in sig_vars:\n",
    "        block = make_graphpad_block(var)  # rows = replicates, columns = groups (A, B, C…)\n",
    "\n",
    "        # Title row: variable name in first cell, rest blank\n",
    "        title_row = pd.DataFrame([[var] + [\"\"] * (block.shape[1] - 1)],\n",
    "                                 columns=block.columns)\n",
    "\n",
    "        # Blank row between variables\n",
    "        blank_row = pd.DataFrame([[\"\"] * block.shape[1]],\n",
    "                                 columns=block.columns)\n",
    "\n",
    "        # Stack: title, table, blank\n",
    "        combined = pd.concat([title_row, block, blank_row], ignore_index=True)\n",
    "        blocks_for_concat.append(combined)\n",
    "\n",
    "    # Stack all variables on top of each other\n",
    "    all_sig_df = pd.concat(blocks_for_concat, ignore_index=True)\n",
    "\n",
    "    output_path = DATA_DIR / \"GraphPad_all_significant_blocks.xlsx\"\n",
    "\n",
    "    with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
    "        all_sig_df.to_excel(writer, sheet_name=\"All_significant\", index=False)\n",
    "\n",
    "    print(f\"✅ Exported all significant variables into one file:\\n  {output_path}\")\n",
    "    print(\"\\nSheet 'All_significant' layout:\")\n",
    "    print(\"  • Title row = variable name\")\n",
    "    print(\"  • Next row = group labels (A, B, C, ...)\")\n",
    "    print(\"  • Following rows = replicates\")\n",
    "    print(\"  • Blank row between variables\")\n",
    "    print(\"\\nTo use in GraphPad:\")\n",
    "    print(\"  1) Open the file\")\n",
    "    print(\"  2) Scroll to the marker you want\")\n",
    "    print(\"  3) Select its group table (title optional)\")\n",
    "    print(\"  4) Copy → Paste into GraphPad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 – Copy a single variable's table to clipboard for quick GraphPad paste\n",
    "\n",
    "def copy_variable_to_clipboard(var: str):\n",
    "    if var not in df_renamed.columns:\n",
    "        raise KeyError(f\"{var!r} is not a column in df_renamed. \"\n",
    "                       f\"Available: {df_renamed.columns.tolist()}\")\n",
    "    \n",
    "    block = make_graphpad_block(df_renamed, var)\n",
    "    print(f\"GraphPad-style table for {var}:\")\n",
    "    display(block)\n",
    "\n",
    "    try:\n",
    "        block.to_clipboard(index=False)\n",
    "        print(\"\\n✅ Table copied to clipboard! You can now paste directly into GraphPad.\")\n",
    "    except Exception as e:\n",
    "        print(\"\\n⚠️ Could not copy to clipboard from this environment.\")\n",
    "        print(\"You can still select the above table manually and copy it.\")\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_vars = anova_df.loc[anova_df[\"p\"] < 0.05, \"variable\"]\n",
    "heat_df = (\n",
    "    df_renamed\n",
    "    .groupby(\"Group\")[list(sig_vars)]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "sns.heatmap(heat_df, annot=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
